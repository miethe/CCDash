---
doc_type: design_spec
status: draft
category: features

title: "Design Spec: Test Visualizer"
description: "Design spec for a test visualizer that shows test status, feature health, and integrity signals."
author: codex
audience: [ai-agents, developers, engineering-leads]
created: 2026-02-28
updated: 2026-02-28

tags: [design, test-visualizer]
feature_slug: test-visualizer
feature_family: test-visualizer
lineage_family: test-visualizer
lineage_parent: ""
lineage_children: []
linked_features: []
related: []
request_log_id: ""
commits: []
prs: []
owner: fullstack-engineering
owners: [fullstack-engineering]
contributors: [ai-agents]
---

## Brief spec overview: CCDash Test & Feature Verification Visualizer

### Context

You’re adding a **test+feature observability subsystem** to CCDash so agentic coding sessions can be judged by **feature health**, not just “tests passing.” It needs live drilldowns, historical forensics (“when did it break?”), and integrity signals (“did an agent weaken the tests?”).

---

## 1) Core capabilities

### A. Live Test Status Page (Domain → Feature → Test → Case)

**Primary UX:** a dedicated CCDash page that shows:

* **Overall health** (global summary + trend)
* **By “Domain”** (a customizable grouping of code/features)
* Drilldown path:

  * Domain
  * Sub-domain(s)
  * Feature(s)
  * Test suite / file
  * Test
  * Test case / parameterized cases (where supported)

**Key requirement:** domains are not hardcoded.

* **Built-in mapping**: heuristics derived from repo structure + test metadata (markers/tags/path conventions) + dependency graph hints.
* **External mapping input**: accept a mapping generated by an LLM semantic pass (or other tool) as *just another mapping provider*.
* **Future UI mapping editor**: users can author/override mappings in-app (and store them as a versioned artifact).

### B. Feature-level View + Historical Forensics

A second view that focuses on **feature health over time**:

* When a feature first became green/red
* When a test was added/edited/removed
* “First bad run” and “last known good” per feature/domain
* Diff-aware integrity checks (test changes correlated to failures)

This view is as much **an audit trail** as it is a dashboard.

---

## 2) Architecture (CCDash-integrated)

### A. Ingestion pipeline

1. **Test results ingestion**

* Parse common formats: JUnit XML (+ optional JSON for richer data)
* Capture: status, duration, stdout/stderr pointers, error signatures, run metadata (commit SHA, branch, agent session id, environment)

2. **Mapping pipeline**

* “Domain Mapping Providers” (pluggable):

  * `RepoHeuristicsProvider` (folder structure, naming conventions)
  * `TestMetadataProvider` (markers/tags/annotations)
  * `SemanticLLMProvider` (imports an externally produced mapping)
  * `UserOverridesProvider` (future UI-authored mapping)
* Provider outputs merge into a **resolved mapping** with precedence rules and conflict reporting.

3. **Integrity pipeline**

* Detect suspicious changes and “green theater” signals:

  * test assertions removed/weakened
  * skips/xfails introduced
  * broad exception catches added
  * tests edited shortly before turning green
* Store integrity signals as first-class metrics.

### B. Query layer (fast, programmatic)

Expose internal APIs (and/or a local SDK) used by both the UI and agentic tooling:

* `GET /health/domains`
* `GET /health/features?domain=...`
* `GET /runs/{run_id}`
* `GET /tests/{test_id}/history`
* `GET /features/{feature_id}/timeline`
* `GET /integrity/alerts?since=...`
* `GET /correlate?run_id=...` (joins against CCDash’s broader correlation system)

---

## 3) Data model (minimal but scalable)

### Entities

* **Run**

  * `run_id`, `timestamp`, `git_sha`, `branch`, `agent_session_id`, `env_fingerprint`, `trigger` (local/CI)
* **TestResult**

  * `run_id`, `test_id`, `status`, `duration_ms`, `error_fingerprint`, `artifact_refs[]`
* **TestDefinition**

  * `test_id` (stable), `path`, `name`, `framework`, `tags/markers`, `owner?`
* **Domain**

  * `domain_id`, `name`, `parent_id?`, `description?`
* **Feature**

  * `feature_id`, `domain_id`, `name`, `parent_id?`, `tier` (core/extras/nonfunc), `priority`
* **Mappings**

  * versioned “resolved mapping” linking `{feature_id/domain_id} ↔ test_id`
  * include source/provider provenance + confidence score
* **IntegritySignals**

  * `git_sha`, `file_path`, `signal_type`, `severity`, `details`, `linked_run_ids[]`

### Derived/rolled-up metrics

* Health rollups per domain/feature with weighting:

  * core > extras > nonfunctional
* Flake indicators (if you track re-runs)
* “Confidence score” per feature: pass-rate *minus* integrity penalties

---

## 4) Storage & efficiency requirements

### Storage strategy

* Treat **full logs/artifacts** as blob references; store **only fingerprints + pointers** in the DB.
* Store **test results as append-only** events keyed by run_id and test_id.
* Keep **mapping snapshots versioned** (compact JSON + hash) so you can reproduce a historical view exactly.

### Performance strategy

* Maintain precomputed rollups:

  * latest status per domain/feature
  * timelines indexed by `feature_id` and `test_id`
* Use fingerprints for joins:

  * `error_fingerprint` groups recurring failures efficiently
  * `env_fingerprint` isolates environment-caused failures

---

## 5) Correlation with the rest of CCDash

This should plug into CCDash’s broader telemetry:

* **Agent session correlation**: which agent run introduced the failure, which prompt/tool actions correlate
* **Repo correlation**: commit, PR, files touched, dependency graph nodes impacted
* **Workflow correlation**: test failures vs implementation-plan milestones vs feature progression states

Net effect: CCDash can answer:

> “Feature X is failing because commit Y changed module Z during agent session S; tests were also edited in a suspicious way 3 minutes earlier.”

